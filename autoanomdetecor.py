# -*- coding: utf-8 -*-
"""AutoAnomDetecor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LQhzUDg2Dv7CvDDBVOxabk3VSuJNzUA

# Automatic Context Anomaly Detector
To Do:
- Make into a case-by-case system
- Takes an input of readings and does the rest from there
- Auto-anom-det-system
- System to get the mean and variance of each sensors readings and use that to generate a data set to train the model
- Try and add as many attacks as possible
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

"""#####Data Set Generator Function"""

#Random Generation of Data Set for testing purposes
def generateDataSet(num_features, readings):
  data_set = []
  #create features according to param
  for i in range(num_features):
    #threshold value
    thresh = np.random.random()
    #randomises variance that is added
    if thresh < 0.26:
      #normal distribution
      data_set.append(list(np.random.normal(20, 2, readings)))
    elif 0.51 > thresh >= 0.26:
      #exponential distribution
      data_set.append(list(np.random.exponential(10, readings)))
    elif 0.76 > thresh >= 0.51:
      #random samples between 0 and 1
      data_set.append(list(np.random.random(readings)))
    else:
      #beta distribution
      data_set.append(list(np.random.beta(5, 4, readings)))
  return (np.array(data_set).transpose(), np.array([0 for i in range(readings)]))

"""##### FDIA Function"""

#False Data Injection Function
def inject(data, mean, std):
  #unpacking tuple as lists
  features = [list(i) for i in data[0]]
  labels = list(data[1])
  #iterating through all features
  for i in range(len(features)):
    #random chance to inject or leave
    if np.random.random() < 0.3333:
      #what values to inject into reading
      values_to_inject = np.random.normal(mean, std, len(features[0]))
      #negating values from indices in row
      features[i] = [a - b for a, b in zip(features[i], values_to_inject)]
      #append 1 for compromised reading
      labels[i] += 1
  #return tuple of updated features and generated labels
  return (np.array(features), np.array(labels))

"""#####Dos Function"""

def dos(data, scale=50):
  #unpacking tuple
  features, labels = list(data[0]), list(data[1])
  #iterate through features
  for i in range(len(features)):
    if np.random.random() < 0.3333:
      #send in that dos
      to_send = list(np.random.random(len(features[0]))*scale)
      #readings insertion
      features.insert(i, to_send)
      #label insertion
      labels.insert(i, 2)
  return (np.array(features), np.array(labels))

"""#####Train test data"""

#generating data
dos_fdia_data = dos(inject(generateDataSet(11, 50), 5, 2))

#Splitting data into train and test data
X_train, X_test, y_train, y_test = train_test_split(dos_fdia_data[0], dos_fdia_data[1], test_size=0.3, random_state=5, shuffle=True)

"""#####Feature Scaling"""

#getting scaler
scaler = StandardScaler()
#transforming train and test features
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""###Machine Learning"""

#initializing model
rf = RandomForestClassifier(n_estimators=100, max_depth=3, criterion='entropy')
#fitting training data
rf.fit(X_train, y_train)

#getting accuracy
y_pred = rf.predict(X_test)
#accuracy score of predictions
print(f"Accuracy of Random Forest: {round(accuracy_score(y_test, y_pred), 2)*100}%")

print(y_test)
print(y_pred)

"""####Load Min Supply"""

#getting data from files
features = np.array(pd.read_csv(r'features.csv', sep=',', header=0))
labels = np.array(pd.read_csv(r'labels.csv', sep=',', header=0)).flatten().astype(int)

#dos attack on data
load_min_tuple = dos((features, labels))
#unpack tuple for model prediction
features, labels = load_min_tuple[0], load_min_tuple[1]

len(labels)

#fitting min supply features for predictions
min_preds = rf.predict(features)
print(len(min_preds))
#comparing classifications
#print(f"Accuracy of Random Forest on LoadMinSupply: {round(accuracy_score(labels, min_preds), 2)*100}%")

