# -*- coding: utf-8 -*-
"""MultiClassDataSetGeneration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X_mfgSUst_rfIP6vzw6k96ssZG4_ha5v

# Generation of Data Sets
"""

import numpy as np

#Each features distribution and creation
f1 = np.random.normal(1, 0.2, 50000)
f2 = np.random.normal(20, 1, 50000)
f3 = np.random.random(50000)
f4 = np.random.normal(50, 30, 50000)
f5 = np.random.normal(2, 0.15, 50000)
f6 = np.random.exponential(2, 50000)
f7 = np.random.normal(0.04, 0.006, 50000)
f8 = np.random.normal(4, 0.03, 50000)
f9 = np.random.normal(15, 7, 50000)
f10 = np.random.normal(56, 12.5, 50000)
f11 = np.random.beta(5, 4, 50000)
f12 = np.random.beta(10, 2, 50000)
f13 = np.random.normal(-4, 0.05, 50000)
f14 = np.random.random(50000)
f15 = np.random.normal(45, 23, 50000)
f16 = np.random.normal(89, 14, 50000)
f17 = np.random.random(50000)
f18 = np.random.normal(150, 34, 50000)
f19 = np.random.normal(2, 0.5, 50000)
f20 = np.random.exponential(10, 50000)

#appending numpy arrays to overall features array
all = []
#loop through global variables appending arrays of features
for i in range(20):
  all.append(globals()[f"f{i+1}"])
#Transposing so each column is now a feature 
all = np.array(all)
features = all.transpose()

"""### FDIA Function
- Takes generated features
- Injects half of the rows
- Chooses random amount of readings to inject
- at random indices
- generates a random gaussian distribution according to params to negate from values
- generates labels as it iterates, 0 for untouched, 1 for compromised
"""

#False Data Injection Function
def inject(features, mean, std):
  #labels initialization
  labels = []
  #iterating through all features
  for row in features:
    #random chance to inject or leave
    if np.random.random() < 0.3333:
      #how many sensors to attack
      sensors_to_attack = np.random.randint(6, 14)
      #what indices to attack
      indices = np.random.choice(range(20), sensors_to_attack, replace=False)
      #what values to inject into indices
      values_to_inject = np.random.normal(mean, std, sensors_to_attack)
      #negating values from indices in row
      row[indices] -= values_to_inject
      #append 1 for compromised reading
      labels.append(1)
    else:
      labels.append(0)
  #return tuple of updated features and generated labels
  return (features, labels)

#tuple of new injected features and labels
inj_tuple = inject(features, 2, 0.5)
features = inj_tuple[0]
labels = inj_tuple[1]

"""###Dos Function
- iterates through all rows
- if random values less than a third
- inject randomly generated reading 
- append label of 2 for DoS
"""

#Convertion of features back to list for speedup
features = [list(i) for i in features]

def Dos(features, labels, scale=1):
  #iterate through features
  for i in range(len(features)):
    if np.random.random() < 0.3333:
      #send in that dos
      to_send = list(np.random.random(20)*scale)
      #insertion
      features.insert(i, to_send)
      #label insertion
      labels.insert(i, 2)
  return (features, labels)

#Simulating DoS and getting final FDIA and DoS data and labels
final_data = Dos(features, labels, scale=50)
features, labels = final_data[0], np.array(final_data[1])

#np arrays for ML
features = np.array(features)
labels = np.array(labels)

#lennies
print(f"length of features: {len(features)}\nlength of labels: {len(labels)}")

"""###Train test data"""

#split for train and test data
from sklearn.model_selection import train_test_split

#Splitting data into train and test data
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=5, shuffle=True)

"""###Feature Scaling
- normalizing data
- subtracts mean and divides by unit variance
"""

#importing scaler
from sklearn.preprocessing import StandardScaler

#getting scaler
scaler = StandardScaler()
#transforming train and test features
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""###Machine Learning"""

#Random Forest for initial test
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

#initializing model
rf = RandomForestClassifier(n_estimators=10, max_depth=2, criterion='entropy', random_state=5)
#fitting training data
rf.fit(X_train, y_train)

#getting accuracy
y_pred = rf.predict(X_test)
#accuracy score of predictions
print(f"Accuracy of Random Forest: {round(accuracy_score(y_test, y_pred), 2)*100}%")

